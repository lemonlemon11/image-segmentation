{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"unet.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"j4xtkT7RKejQ","colab_type":"code","outputId":"45bdecc2-79bf-4cb7-fe4f-19a7cd4aeb3e","executionInfo":{"status":"ok","timestamp":1565582822533,"user_tz":-600,"elapsed":5711,"user":{"displayName":"吴恒星","photoUrl":"","userId":"14038288694297837157"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["! pip install tifffile"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: tifffile in /usr/local/lib/python3.6/dist-packages (2019.7.26)\n","Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from tifffile) (1.16.4)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"UkoGySuJ-IZ8","colab_type":"text"},"source":["# This is the class for creating K fold validation data"]},{"cell_type":"code","metadata":{"id":"uBuYhmalFDOP","colab_type":"code","colab":{}},"source":["import numpy as np\n","import skimage.io as io\n","from sklearn.model_selection import train_test_split, StratifiedKFold\n","from sklearn.model_selection import KFold\n","import os"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6VIJyD089RDi","colab_type":"code","colab":{}},"source":["class K_fold_validataion():\n","    def __init__(self,root='/content/drive/My Drive/'):\n","        self.root=root\n","        if os.path.exists(self.root+'data1/1_fold'):\n","            print('K fold validation data has exsisted. No need to create folders')\n","            return \n","        else:\n","            self.mkSubFold(5)\n","        \n","    def findAllData(self):\n","        alltrain = io.imread(self.root+\"data1/images/train-volume00.jpg\",as_gray=True)\n","        alllabel = io.imread(self.root+\"data1/labels/train-labels00.jpg\",as_gray=True)\n","        alltrain = np.reshape(alltrain,alltrain.shape+(1,))\n","        alllabel = np.reshape(alllabel,alllabel.shape+(1,))\n","    #     print(alltrain.shape)\n","    #     print(alllabel.shape)\n","        for i in range(1,30):\n","            eachimg = io.imread(self.root+\"data1/images/train-volume\"+str(i).zfill(2)+\".jpg\",as_gray=True)\n","            eachlabel = io.imread(self.root+\"data1/labels/train-labels\"+str(i).zfill(2)+\".jpg\",as_gray=True)\n","            eachimg = np.reshape(eachimg,eachimg.shape+(1,))\n","            eachlabel = np.reshape(eachlabel,eachlabel.shape+(1,))\n","            alltrain = np.concatenate([alltrain,eachimg],axis = -1)\n","            alllabel = np.concatenate([alllabel,eachlabel],axis = -1)\n","        print(alltrain.shape)\n","        print(alllabel.shape)\n","        return alltrain,alllabel\n","      \n","    def makeDir(self,save_path):\n","        os.makedirs(self.root+save_path+\"train/images/\")\n","        os.makedirs(self.root+save_path+\"train/labels/\")\n","        os.makedirs(self.root+save_path+\"validation/images/\")\n","        os.makedirs(self.root+save_path+\"validation/labels/\")\n","        os.makedirs(self.root+save_path+\"jpg\")\n","        os.makedirs(self.root+save_path+\"tif\")\n","    def writeData(self,train_img,train_label,val_img,val_label,foldNum):\n","        save_path = \"data1/\"+str(foldNum)+\"_fold/\"\n","        self.makeDir(save_path)\n","        for i in range(train_img.shape[2]):\n","            io.imsave(self.root+save_path+\"train/images/\"+str(i).zfill(2)+\".jpg\",train_img[:,:,i],quality=100)\n","            io.imsave(self.root+save_path+\"train/labels/\"+str(i).zfill(2)+\".jpg\",train_label[:,:,i],quality=100)\n","        for i in range(val_img.shape[2]):\n","            io.imsave(self.root+save_path+\"validation/images/\"+str(i).zfill(2)+\".jpg\",val_img[:,:,i],quality=100)\n","            io.imsave(self.root+save_path+\"validation/labels/\"+str(i).zfill(2)+\".jpg\",val_label[:,:,i],quality=100)\n","     \n","    def mkSubFold(self,k):\n","        alltrain,alllabel = self.findAllData()\n","        indices = np.arange(alltrain.shape[2])\n","    #     np.random.shuffle(indices)\n","    #     alltrain = alltrain[:,:,indices]\n","    #     alllabel = alllabel[:,:,indices]\n","        numPairsData = len(indices)\n","        kf = KFold(n_splits=k,shuffle=True,random_state=1)\n","        foldNum = 1\n","        for trainIdx,valIdx in kf.split(indices):\n","            train_img = alltrain[:,:,trainIdx]\n","            train_label = alllabel[:,:,trainIdx]\n","            val_img = alltrain[:,:,valIdx]\n","            val_label = alllabel[:,:,valIdx]\n","            self.writeData(train_img,train_label,val_img,val_label,foldNum)\n","            foldNum+=1 \n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"xKOcNKsWFBtk","colab_type":"code","outputId":"bc213923-024c-4978-80cd-96c5c3fe8ca7","executionInfo":{"status":"ok","timestamp":1565582825632,"user_tz":-600,"elapsed":654,"user":{"displayName":"吴恒星","photoUrl":"","userId":"14038288694297837157"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["k_fold=K_fold_validataion(root='/content/drive/My Drive/')\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["K fold validation data has exsisted. No need to create folders\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"9DRa2fpM_fai","colab_type":"text"},"source":["# The following part is the training part"]},{"cell_type":"code","metadata":{"id":"VQ_EVemdH9pN","colab_type":"code","outputId":"a4ca314d-ce70-42cb-c185-12ef9888b915","executionInfo":{"status":"ok","timestamp":1565580921321,"user_tz":-600,"elapsed":3684,"user":{"displayName":"吴恒星","photoUrl":"","userId":"14038288694297837157"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from keras.optimizers import *\n","from keras.layers import Conv2D,MaxPooling2D,Dense,Dropout,UpSampling2D,concatenate,Input,BatchNormalization,LeakyReLU,Conv2DTranspose\n","from keras.models import Sequential,Model\n","from keras.preprocessing.image import ImageDataGenerator,load_img,img_to_array,array_to_img\n","from keras.callbacks import ModelCheckpoint,EarlyStopping,ReduceLROnPlateau\n","import os\n","import glob\n","import numpy as np\n","import time\n","import random\n","from matplotlib import pyplot as plt\n","from tifffile import imsave"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"1xfWs_466Uuh","colab_type":"text"},"source":["## Use classes to encapsulate different methods\n"]},{"cell_type":"code","metadata":{"id":"IRzcHaSDIEQh","colab_type":"code","outputId":"ac4d8597-a82b-4329-833f-718b8f9b5142","executionInfo":{"status":"ok","timestamp":1565581119159,"user_tz":-600,"elapsed":140760,"user":{"displayName":"吴恒星","photoUrl":"","userId":"14038288694297837157"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["class Unet():\n","  \n","    # initialize parameters\n","    def __init__(self,img_height=512,img_width=512,data_path='data1',img_type='jpg',test_img_path='test_images',result_img_path='result_images'):\n","        self.img_height=img_height\n","        self.img_width=img_width\n","        self.data_path=data_path\n","        self.img_type=img_type\n","        self.npy_path=data_path\n","        self.test_img_path=test_img_path\n","        self.result_img_path=result_img_path\n","        self.tif='tif'\n","    \n","    # This method is used for offline training to generate more training images,\n","    # but found that offline training is not a good method to train the network\n","    # Aftering doing a lot of experiments, I use online training instead of offline training\n","    def load_data_to_gen_more_img(self):\n","        datagen = ImageDataGenerator(\n","            featurewise_center=True,\n","            featurewise_std_normalization=True,\n","            rotation_range=20,\n","            width_shift_range=0.2,\n","            height_shift_range=0.2,\n","            horizontal_flip=True\n","        )\n","        train_images=glob.glob(self.data_path+'/images/*.'+self.img_type)\n","        train_images=sorted(train_images)\n","        test_images=glob.glob(self.data_path+'/labels/*'+self.img_type)\n","        test_images=sorted(test_images)\n","        train_images_data_list=[]\n","        test_images_data_list=[]\n","        print(train_images_data_list)\n","        for idx in range(len(train_images)):\n","            train_images_data_list.append(img_to_array(load_img(train_images[idx])))\n","            test_images_data_list.append(img_to_array(load_img(test_images[idx])))\n","            \n","        test_images_data_arr=np.array(test_images_data_list)\n","\n","        train_images_data_arr=np.array(train_images_data_list)\n","        \n","        gen_data_train = datagen.flow(train_images_data_arr, batch_size=30, shuffle=False, save_to_dir=self.data_path+'/trans_images',\n","                                               save_prefix='trans-train',save_format='jpg',seed=1)\n","    \n","        gen_data_labels = datagen.flow(test_images_data_arr, batch_size=30, shuffle=False, save_to_dir=self.data_path+'/trans_labels',\n","                                               save_prefix='trans-labels',save_format='jpg',seed=1)\n","        for i in range(10):\n","            print('gens '+ str(30*(1+i)) +' images')\n","            gen_data_train.next()\n","            gen_data_labels.next()\n","    \n","    # This is used for renaming the images in offline training\n","    def rename_file(self):\n","        train_path =self.data_path+'/trans_images'\n","        test_path =self.data_path+'/trans_labels'\n","        files=sorted(os.listdir(train_path))\n","        files1=sorted(os.listdir(test_path))\n","        print('num of trans_images ',len(files))\n","        print('num of trans_labels ',len(files1))\n","        if len(files1)!=len(files):\n","            sys.exit()\n","        for idx in range(len(files)):\n","            if (idx+1)%100==0:\n","                print('rename'+ str(idx+1)+ 'images')\n","            if files[idx]!='.DS_Store':\n","                filename=str(idx)\n","                os.rename(os.path.join(train_path,files[idx]),os.path.join(train_path,filename+\".\"+self.img_type))\n","            if files1[idx]!='.DS_Store':\n","                filename=str(idx)\n","                os.rename(os.path.join(test_path,files1[idx]),os.path.join(test_path,filename+\".\"+self.img_type))\n","    \n","   # A part of offline training\n","    def create_train_data(self):\n","#         if os.path.exists(self.data_path+'/image_train.npy') or os.path.exists(self.data_path+'/image_labels.npy'):\n","#             print('training data npy already exists...........')\n","#             return\n","        print('loading data.........................')\n","        train_images=glob.glob(self.data_path+'/images/*.'+self.img_type)\n","        train_images=sorted(train_images)\n","        train_images_trans=glob.glob(self.data_path+'/trans_images/*'+self.img_type)\n","        train_images_trans=sorted(train_images_trans)\n","\n","        train_images_data_list=[]\n","        \n","        train_image_paths=train_images_trans+train_images\n","\n","        imgdatas_train=np.ndarray((len(train_image_paths),512,512,1),dtype=np.float32)\n","        count=0\n","        for path in train_image_paths:\n","            img=load_img(path,color_mode='grayscale')\n","            img=img_to_array(img)\n","            img_train=img/255\n","\n","            imgdatas_train[count]=img_train\n","            if count%100==0:\n","                print('create data '+str(count)+' images')\n","            count+=1\n","        np.save(self.npy_path+'/image_train.npy',imgdatas_train)\n","        \n","        test_images=glob.glob(self.data_path+'/labels/*.'+self.img_type)\n","        test_images=sorted(test_images)\n","        \n","        test_images_trans=glob.glob(self.data_path+'/trans_labels/*'+self.img_type)\n","        test_images_trans=sorted(test_images_trans)\n","        \n","        test_images_data_list=[] \n","        test_image_paths=test_images_trans+test_images\n","        imgdatas_test=np.ndarray((len(test_image_paths),self.img_height,self.img_width,1),dtype=np.float32)\n","        count1=0\n","        for path in test_image_paths:\n","            img=load_img(path,color_mode = \"grayscale\")\n","            img=img_to_array(img)\n","            if count1%100==0:\n","                print('create data '+str(count1)+' labels')\n","#             print('load data',img_train.shape,img_label.shape)\n","            img_label=img/255\n","\n","            img_label[img_label > 0.5] = 1.0\n","            img_label[img_label <= 0.5] = 0.0\n","            \n","            imgdatas_test[count1]=img_label\n","            count1+=1\n","        np.save(self.npy_path+'/image_labels.npy',imgdatas_test)\n","        print('finish loading data.............................')\n","  \n","    # A part of offline training\n","    def load_data_to_train(self):\n","        img_train=np.load(self.npy_path+'/image_train.npy')\n","        img_label=np.load(self.npy_path+'/image_labels.npy')\n","        return img_train,img_label\n","    \n","    # Building the network model\n","    def create_unet(self):\n","        inputs=Input((self.img_height,self.img_width,1))\n","        conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(inputs)\n","        conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv1)\n","        pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n","        conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool1)\n","        conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv2)\n","        pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n","        conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool2)\n","        conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv3)\n","        pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n","        conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool3)\n","        conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv4)\n","        drop4 = Dropout(0.7)(conv4)\n","        pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)\n","\n","        conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool4)\n","        conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv5)\n","        drop5 = Dropout(0.7)(conv5)\n","\n","        up6 = Conv2D(512, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(drop5))\n","        merge6 = concatenate([drop4,up6], axis = 3)\n","        conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge6)\n","        conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv6)\n","        conv6 = Dropout(0.7)(conv6)\n","        \n","        up7 = Conv2D(256, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv6))\n","        merge7 = concatenate([conv3,up7], axis = 3)\n","        conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge7)\n","        conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv7)\n","        conv7 = Dropout(0.7)(conv7)\n","        \n","        up8 = Conv2D(128, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv7))\n","        merge8 = concatenate([conv2,up8], axis = 3)\n","        conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge8)\n","        conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv8)\n","        conv8 = Dropout(0.7)(conv8)\n","        \n","        up9 = Conv2D(64, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv8))\n","        merge9 = concatenate([conv1,up9], axis = 3)\n","        conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge9)\n","        conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n","        conv9 = Conv2D(2, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n","        conv10 = Conv2D(1, 1, activation = 'sigmoid')(conv9)\n","        \n","        model=Model(input=inputs,output=conv10)\n","        return model\n","    \n","    # Using offline training method to train the network\n","    def train(self):\n","        if os.path.exists(self.data_path+'/unet.hdf5'):\n","            print('file exists, not need to train....................')\n","            model=self.create_unet()\n","            model.load_weights(self.data_path+'/unet.hdf5')\n","            \n","            predict_imgs=np.load(self.npy_path+'/predict_imgs.npy')\n","            result=model.predict(predict_imgs,batch_size=1,verbose=1)\n","            \n","            np.save(self.npy_path+'/predict_imgs_result.npy',result)\n","            \n","        else:\n","            print('loading data to train the model.................')\n","            img_train,img_label=self.load_data_to_train()\n","            print('img_train',img_train.shape)\n","            print('img_label',img_label.shape)\n","\n","            \n","            check_point=ModelCheckpoint(self.data_path+'/unet.hdf5',monitor='val_loss',verbose=1,save_best_only=True)\n","            early_stop=EarlyStopping(monitor='val_loss', patience=3, verbose=1, mode='auto')\n","            schedular_unet=ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=2, verbose=1, mode='auto', epsilon=0.0001, cooldown=0, min_lr=0)\n","            model=self.create_unet()\n","            model.compile(optimizer=Adam(lr=0.0001),loss='binary_crossentropy',metrics=['accuracy'])\n","\n","            history=model.fit(img_train,img_label,batch_size=2,nb_epoch=10,verbose=1,validation_split=0.2,shuffle=True,callbacks=[schedular_unet,check_point,early_stop])\n","            self.plt_hist(history)\n","\n","            with open(self.data_path+'/log_unet_1000.txt','w') as f:\n","              f.write(str(history.history))\n","            np.save(self.npy_path+'/mdoel',model)\n","\n","            predict_imgs=np.load(self.npy_path+'/predict_imgs.npy')\n","\n","            result=model.predict(predict_imgs,batch_size=1,verbose=1)\n","            np.save(self.npy_path+'/predict_imgs_result.npy',result)\n","    \n","    # Plot the training history image and save it in the specific folder\n","    def plt_hist(self,history,folder_path):\n","        plt.plot(history.history['loss'])\n","        plt.plot(history.history['val_loss'])\n","        plt.plot(history.history['val_acc'])\n","        plt.plot(history.history['acc'])\n","        plt.title(\"model loss\")\n","        plt.ylabel(\"loss\")\n","        plt.xlabel(\"epoch\")\n","        plt.legend([\"loss\",\"val_loss\",\"val_acc\",\"acc\"],loc=\"upper left\")\n","        current_time=time.strftime(\"%Y%m%d%H%M%S\", time.localtime())\n","\n","        plt.savefig(folder_path+'/val_loss_'+current_time+'.jpg')\n","        plt.show()\n","    \n","    # Save the test image to the specified folder\n","    def save_img(self,folder_path):\n","        result_npy=np.load(self.npy_path+'/predict_imgs_result.npy')\n","        for i in range(result_npy.shape[0]):\n","            img=result_npy[i]\n","#             print(np.max(img))\n","#             max_value=np.max(img)\n","#             img[img/max_value > 0.5] = 255\n","#             img[img/max_value <= 0.5] = 0\n","#             img[img>0.5]=1\n","#             img[img<=0.5]=0\n","#             img_tif=tifffile.imsave(img)\n","            img2=array_to_img(img)\n","            print(i)\n","            img2.save(folder_path+'/jpg/'+str(i)+'.'+self.img_type)\n","            imsave(folder_path+'/tif/'+str(i)+'.'+self.tif,img)\n","    \n","    # Loding test images and create a npy file to store the test images\n","    def predict_img(self):\n","        if os.path.exists(self.data_path+'/predict_imgs.npy'):\n","            print('predict data npy has already exists..................')\n","            return\n","        selected_paths=sorted(glob.glob(self.data_path+'/'+self.test_img_path+'/*.'+self.img_type))\n","        predict_images=np.ndarray((len(selected_paths),self.img_height,self.img_width,1),dtype=np.int)\n","        \n","        for idx in range(len(selected_paths)):\n","            img=load_img(selected_paths[idx],color_mode='grayscale')\n","            img=img_to_array(img)\n","            predict_images[idx]=img\n","        np.save(self.npy_path+'/predict_imgs.npy',predict_images)\n","        print('creating predict images finish........................')\n","    \n","    # K flod validation (5 fold validation)\n","    def K_fold_val(self):\n","        model=self.create_unet()\n","        files=glob.glob(self.data_path+'/*')\n","        K_fold_paths=[obj for obj in files if obj.endswith('_fold')]\n","        for folder_path in K_fold_paths:\n","            train_path=folder_path+'/train'\n","            val_path=folder_path+'/validation'\n","            self.train_fit_gen(folder_path,train_path,val_path,model)\n","\n","        return K_fold_paths\n","      \n","    # Using online training to train the model\n","    def train_fit_gen(self,folder_path,train_path,val_path,model):\n","        if os.path.exists(folder_path+'/unet_fit_gen.hdf5'):\n","            print('file exists, not need to train....................')\n","            model=self.create_unet()\n","            model.load_weights(folder_path+'/unet_fit_gen.hdf5')\n","            \n","            predict_imgs=np.load(self.npy_path+'/predict_imgs.npy')\n","            result=model.predict(predict_imgs,batch_size=1,verbose=1)\n","            \n","            np.save(self.npy_path+'/predict_imgs_result.npy',result)\n","            self.save_img(folder_path)\n","            return \n","        \n","        data_gen_args = dict(rotation_range=0.2,\n","                    width_shift_range=0.1,\n","                    height_shift_range=0.1,\n","                    shear_range=0.1,\n","                    zoom_range=0.1,\n","                    horizontal_flip=True,\n","                    fill_mode='nearest')\n","        \n","        \n","        early_stop=EarlyStopping(monitor='val_loss', patience=3, verbose=1, mode='auto')\n","        myGene = self.trainGenerator(2,train_path,'images','labels',data_gen_args)\n","        schedular_unet=ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=2, verbose=1, mode='auto', epsilon=0.0001, cooldown=0, min_lr=0)\n","        \n","        model.compile(optimizer=Adam(lr=0.0002),loss='binary_crossentropy',metrics=['accuracy'])\n","        model_checkpoint = ModelCheckpoint(folder_path+'/unet_fit_gen.hdf5', monitor='val_loss',verbose=1, save_best_only=True,mode='auto')\n","\n","        myGene_test = self.trainGenerator(2,val_path,'images','labels',data_gen_args,seed=2)\n","\n","        history=model.fit_generator(generator=myGene,validation_data=myGene_test,validation_steps=10,steps_per_epoch=1000,epochs=10,callbacks=[early_stop,schedular_unet,model_checkpoint],verbose=1,class_weight='auto')\n","        self.plt_hist(history,folder_path)\n","\n","        with open(folder_path+'/log_unet.txt','w') as f:\n","            f.write(str(history.history))\n","        np.save(folder_path+'/mdoel.h5',model)\n","\n","#         predict_imgs=np.load(self.npy_path+'/predict_imgs.npy')\n","\n","#         result=model.predict(predict_imgs,batch_size=1,verbose=1)\n","        \n","#         np.save(self.npy_path+'/predict_imgs_result.npy',result)\n","    \n","    # Create a image generator to create augmented images\n","    def trainGenerator(self,batch_size,train_path,image_folder,label_folder,aug_dict,image_color_mode = \"grayscale\",\n","                    label_color_mode = \"grayscale\",target_size = (512,512),seed = 1):\n","  \n","        image_datagen = ImageDataGenerator(**aug_dict)\n","        label_datagen = ImageDataGenerator(**aug_dict)\n","        image_generator = image_datagen.flow_from_directory(\n","            train_path,\n","            classes = [image_folder],\n","            class_mode = None,\n","            color_mode = image_color_mode,\n","            target_size = target_size,\n","            batch_size = batch_size,\n","            seed = seed)\n","        label_generator = label_datagen.flow_from_directory(\n","            train_path,\n","            classes = [label_folder],\n","            class_mode = None,\n","            color_mode = label_color_mode,\n","            target_size = target_size,\n","            batch_size = batch_size,\n","            seed = seed)\n","        train_generator = zip(image_generator, label_generator)\n","        for (img,label) in train_generator:\n","            if np.max(img)>1:\n","              img = img / 255\n","              label = label /255\n","              label[label > 0.5] = 1\n","              label[label <= 0.5] = 0\n","            yield (img,label)\n","            \n","    \n","      \n","      \n","net=Unet(data_path='/content/drive/My Drive/data1',img_height=512,img_width=512,img_type='jpg',test_img_path='test_images',result_img_path='result_images')\n","\n","# net.load_data_to_gen_more_img()\n","# net.rename_file()\n","# net.create_train_data()\n","# net.predict_img()\n","# net.train_fit_gen()\n","# net.train()\n","# net.save_img()\n","net.K_fold_val()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:169: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"co...)`\n"],"name":"stderr"},{"output_type":"stream","text":["file exists, not need to train....................\n","30/30 [==============================] - 16s 543ms/step\n","0\n","1\n","2\n","3\n","4\n","5\n","6\n","7\n","8\n","9\n","10\n","11\n","12\n","13\n","14\n","15\n","16\n","17\n","18\n","19\n","20\n","21\n","22\n","23\n","24\n","25\n","26\n","27\n","28\n","29\n","file exists, not need to train....................\n","30/30 [==============================] - 8s 270ms/step\n","0\n","1\n","2\n","3\n","4\n","5\n","6\n","7\n","8\n","9\n","10\n","11\n","12\n","13\n","14\n","15\n","16\n","17\n","18\n","19\n","20\n","21\n","22\n","23\n","24\n","25\n","26\n","27\n","28\n","29\n","file exists, not need to train....................\n","30/30 [==============================] - 8s 267ms/step\n","0\n","1\n","2\n","3\n","4\n","5\n","6\n","7\n","8\n","9\n","10\n","11\n","12\n","13\n","14\n","15\n","16\n","17\n","18\n","19\n","20\n","21\n","22\n","23\n","24\n","25\n","26\n","27\n","28\n","29\n","file exists, not need to train....................\n","30/30 [==============================] - 8s 272ms/step\n","0\n","1\n","2\n","3\n","4\n","5\n","6\n","7\n","8\n","9\n","10\n","11\n","12\n","13\n","14\n","15\n","16\n","17\n","18\n","19\n","20\n","21\n","22\n","23\n","24\n","25\n","26\n","27\n","28\n","29\n","file exists, not need to train....................\n","30/30 [==============================] - 8s 272ms/step\n","0\n","1\n","2\n","3\n","4\n","5\n","6\n","7\n","8\n","9\n","10\n","11\n","12\n","13\n","14\n","15\n","16\n","17\n","18\n","19\n","20\n","21\n","22\n","23\n","24\n","25\n","26\n","27\n","28\n","29\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["['/content/drive/My Drive/data1/1_fold',\n"," '/content/drive/My Drive/data1/4_fold',\n"," '/content/drive/My Drive/data1/3_fold',\n"," '/content/drive/My Drive/data1/2_fold',\n"," '/content/drive/My Drive/data1/5_fold']"]},"metadata":{"tags":[]},"execution_count":25}]},{"cell_type":"code","metadata":{"id":"1q83-_y2-9fs","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}